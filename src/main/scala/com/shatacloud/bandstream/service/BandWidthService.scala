package com.shatacloud.bandstream.service

import com.shatacloud.bandstream.model.LogRecordModel
import com.shatacloud.bandstream.util.SparkSessionSingleton
import org.apache.kafka.clients.consumer.ConsumerRecord
import org.apache.kafka.common.TopicPartition
import org.apache.spark.TaskContext
import org.apache.spark.rdd.RDD
import org.apache.spark.sql.{DataFrame, Row}
import org.apache.spark.streaming.StreamingContext
import org.apache.spark.streaming.kafka010.{ConsumerStrategies, HasOffsetRanges, KafkaUtils, LocationStrategies}
import scalikejdbc.{DB, SQL}


object BandWidthService {

  /**
    * Retrieve committed kafka topic, parititon_num, from_offset from DB
    *
    * @return kafka topic, parititon_num, from_offset
    */
  def getOffsetFromDB: Map[TopicPartition, Long] = {
    DB.readOnly { implicit session =>
      SQL("""select topic, partition_num, from_offset, until_offset from kafka_offset""")
        .map { resultSet =>
          new TopicPartition(resultSet.string(1), resultSet.int(2)) -> resultSet.long(3)
        }.list.apply().toMap
    }
  }

  /**
    * Use spark Direct Kafka API to create DStream.
    * Aggregate(SUM) traffic per batch
    *
    * @param ssc           StreamingContext
    * @param dbFromOffsets offsetMap retrieved from DB
    * @param kafkaParams   kakfka configuration params
    * @return ssc StreamingContext
    */
  def createAndAggregateStream(ssc: StreamingContext, dbFromOffsets: Map[TopicPartition, Long], kafkaParams: Map[String, Object]): StreamingContext = {
    //val kafkaStream08 = org.apache.spark.streaming.kafka.KafkaUtils.createDirectStream[String,String](ssc, kafkaParams, topics)
    //val logStream08 = kafkaStream08.map(e => e._2).map(line => line.split("\\|"))
    val stream = KafkaUtils.createDirectStream[String, String](
      ssc,
      LocationStrategies.PreferConsistent,
      ConsumerStrategies.Assign[String, String](dbFromOffsets.keys.toList, kafkaParams, dbFromOffsets)
    )

    stream.foreachRDD { (rdd, time) =>
      val offsetRanges = rdd.asInstanceOf[HasOffsetRanges]
      println(s"offset get from kafka ${offsetRanges.offsetRanges.mkString(",")}")

      val partitionId = TaskContext.getPartitionId() + 1
      val uniqueId = this.generateSubmitId(time.milliseconds, partitionId)
      println(s"partitionId is $partitionId. uniqueId is $uniqueId")

      val results = this.aggregateBandWidth(rdd).collect()

      // Back to running on the driver
      this.transactionSavePerBatch(results, offsetRanges, uniqueId)
    }
    ssc
  }

  private def generateSubmitId(milliseconds: Long, partitionId: Int): BigInt = milliseconds + partitionId

  /**
    * Use sparkSQL to calc tarffic and bandwidth with aggregation operation SUM
    *
    * @param rdd a kafkaRDD get from Direct Kafka API
    * @return a dataFrame that wraps SQL calculation result
    */
  private def aggregateBandWidth(rdd: RDD[ConsumerRecord[String, String]]): DataFrame = {
    val sparkSession = SparkSessionSingleton.getInstance(rdd.sparkContext.getConf)
    import sparkSession.implicits._
    val recordDataFrame = rdd
      .map(record => record.value().split("\\|"))
      .map(e => LogRecordModel(e(0), e(1), e(2).toDouble, e(3), e(4), e(5).toInt, e(6).toDouble, e(7).toInt, e(8).toLong, e(9),
        e(10).toInt, e(11), e(12), e(13), e(14), e(15).toDouble, e(16).toInt, e(17), e(18), e(19).toDouble, e(20), e(21).toDouble,
        e(22), e(23), e(24), e(25).toDouble, e(26).toLong, e(27).toInt, e(28), e(29), e(30), e(31), e(32).toLong, e(33).toDouble,
        e(34), e(35), e(36), e(37), e(38)))
      .toDF()

    recordDataFrame.createOrReplaceTempView("node_bandwidth_events")

    val sql = "select sp_channel, device_node as node_tag, " +
      "cast(to_date(from_unixtime(cast(msec as bigint) div 300 * 300)) as String) as record_date, " +
      "from_unixtime(cast(msec as BIGINT) div 300 * 300) AS record_time, " +
      "sum(segment_bytes_sent + case when segment_type = 2 then request_length else 0 end) AS traffic, " +
      "sum(segment_bytes_sent + case when segment_type = 2 then request_length else 0 end)*8/300/1000/1000/1000 AS bandwidth " +
      "from node_bandwidth_events " +
      "where request_method != 'HEAD' and request_method != 'PURGE' and segment_type != 0 " +
      "group by sp_channel, device_node, from_unixtime(cast(msec as BIGINT) div 300 * 300)"

    val resultDataFrame = sparkSession.sql(sql)
    resultDataFrame
  }

  /**
    * DB.localTx is transactional, if business update or offset update fails, neither will be committed
    * store business results and kafka offsets transactionally
    *
    * @param results      business calculation result
    * @param offsetRanges HasOffsetRanges that wraps a private OffsetRanges get from Direct Kafka API
    * @param submitId     a unique id generated by timestamp and partition id
    */
  private def transactionSavePerBatch(results: Array[Row], offsetRanges: HasOffsetRanges, submitId: BigInt) = {
    DB.localTx { implicit session =>
      results.foreach { row =>
        println(s"${row.getString(0)}, ${row.getString(1)}, ${row.getString(2)}, ${row.getString(3)}, ${row.getDecimal(4)}, ${row.getDecimal(5)}")
        val metricRows =
          SQL("""insert into log_record values(?,?,?,?,?,?,?)""")
            .bind(submitId, row.getString(0), row.getString(1), row.getString(2), row.getString(3), row.getDecimal(4), row.getDecimal(5))
            .update()
            .apply()
        if (metricRows != 1) {
          throw new IllegalStateException(s"""Got $metricRows rows affected instead of 1 when attempting to update bandwidth calculation for submitId [$submitId]""")
        }
      }

      offsetRanges.offsetRanges.foreach { osr =>
        val offsetRows =
          SQL("""update kafka_offset set from_offset = ? where topic = ? and partition_num = ? and from_offset = ?""")
            .bind(osr.untilOffset, osr.topic, osr.partition, osr.fromOffset)
            .update
            .apply()
        if (offsetRows != 1) {
          throw new IllegalStateException(s"""Got $offsetRows rows affected instead of 1 when attempting to update kafka offsets in DB for ${osr.topic} ${osr.partition} ${osr.fromOffset} -> ${osr.untilOffset}.Was a partition repeated after a worker failure?""".stripMargin)
        }
      }
    }
  }
}
